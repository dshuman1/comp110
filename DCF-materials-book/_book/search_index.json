[
["gettingorganized.html", "Notes and Materials for Data Computing § 1 (PART) Getting Organized", " Notes and Materials for Data Computing Daniel Kaplan 2016-09-28 § 1 (PART) Getting Organized The materials from Week 1 will go here. Getting to RStudio Connecting to GitHub RMarkdown Writing a simple RMarkdown document One-page cheat sheet. "],
["datainfrastructure.html", "§ 2 (PART) Data Infrastructure Topics", " § 2 (PART) Data Infrastructure Topics The structure of tabular data cases and variables numerical and categorical variables tidy data R Commands Files and documents "],
["case-study-highway-fatalities.html", "§ 3 Case Study: Highway Fatalities 3.1 The accident data 3.2 Other tables", " § 3 Case Study: Highway Fatalities On August 29, 2016, the White House issued a data-science “call to action.”1(https://www.transportation.gov/fastlane/2015-traffic-fatalities-data-has-just-been-released-call-action-download-and-analyze). Today, the U.S. Department of Transportation is releasing an open data set that contains detailed, anonymized information about each of these tragic incidents. As the new data being released show, and as DOT reported earlier this summer, 2015 showed a marked increase in traffic fatalities nationwide. To be precise, 7.2% more people died in traffic-related accidents in 2015 than in 2014. This unfortunate data point breaks a recent historical trend of fewer deaths occurring per year. Under the leadership of Transportation Secretary Anthony Foxx, we’re doing two things differently this year. One: We’re publishing the data through NHTSA’s Fatality Analysis Reporting System (FARS) three months earlier than last year. Two: We’re directly soliciting your help to better understand what these data are telling us. Whether you’re a non-profit, a tech company, or just a curious citizen wanting to contribute to the conversation in your local community, we want you to jump in and help us understand what the data are telling us. Some key questions worth exploring: How might improving economic conditions around the country change how Americans are getting around? What models can we develop to identify communities that might be at a higher risk for fatal crashes? How might climate change increase the risk of fatal crashes in a community? How might we use studies of attitudes toward speeding, distracted driving, and seat belt use to better target marketing and behavioral change campaigns? How might we monitor public health indicators and behavior risk indicators to target communities that might have a high prevalence of behaviors linked with fatal crashes (drinking, drug use/addiction, etc.)? DOT is aggressively seeking ways to improve safety on the roads. From our work with the auto industry to improve vehicle safety, to new solutions to behavioral challenges like drunk, drugged, distracted and drowsy driving, we know we need to find novel solutions to old challenges. We’re also looking to accelerate technologies that may make driving safer, including connected and highly automated vehicles. But we need your help, too! Data Science is a team sport. We are calling on data scientists, public health experts, students and researchers—even if you have never thought about road safety before—to dive in to these data and help answer these important questions, especially on tough issues like pedestrian and bicyclist fatalities. Start by downloading and playing with the data. Then share your insights and let us know what you find by sending us a note at opendata@dot.gov. 3.1 The accident data The link to the data in the call to action is ftp://ftp.nhtsa.dot.gov/fars/2015/.2 Go to that site. Is it immediately clear what’s going on? What can you figure out by browsing the site. look in “parent” directories try substituting 2015 for other similar sorts of values in the URL. A blog by Lucas Puente conveniently gives simple instructions for downloading the data. He writes: Simply visit ftp://ftp.nhtsa.dot.gov/fars/2015/National/ and download the FARS2015NationalDBF.zip file, unzip it, and load into R. After unzipping, there is a directory FARS2015NationalDBF taking up 874.9 MB of disk for 27 items. I put it in my Downloads directory. Lucas provides commands to read the data into R. library(foreign) accidents &lt;- read.dbf(&quot;FARS2015NationalDBF/accident.dbf&quot;) To make sense of these instructions, it helps to know some things: What is library(foreign) doing? What does the library() part of the command tell you? What is foreign. How would get get some instructions or documentation for foreign to help you understand why this is appropriate. What is read.dbf()? What does &quot;FARS2015NationalDBF/accident.dbf&quot; tell you about the data file and where it’s located. Aside: I would rather you wrote: filename &lt;- &quot;~/Downloads/FARS2015NationalDBF/accident.dbf&quot; Accidents &lt;- foreign::read.dbf(filename) What’s different about the file name I’m using? Why? Lucas’s blog leads you through the steps of making a map of accident locations: Lucas’s map What does this map tell you? With the Accidents data table read into R, it’s easy to look at it and perhaps construct summaries. ## STATE ST_CASE VE_TOTAL VE_FORMS PVH_INVL PEDS PERNOTMVIT PERMVIT ## 1 1 10001 1 1 0 0 0 1 ## 2 1 10002 1 1 0 0 0 1 ## 3 1 10003 1 1 0 0 0 2 ## 4 1 10004 1 1 0 0 0 1 Some simple things: nrow(Accidents) ## [1] 32166 names(Accidents) ## [1] &quot;STATE&quot; &quot;ST_CASE&quot; &quot;VE_TOTAL&quot; &quot;VE_FORMS&quot; &quot;PVH_INVL&quot; ## [6] &quot;PEDS&quot; &quot;PERNOTMVIT&quot; &quot;PERMVIT&quot; &quot;PERSONS&quot; &quot;COUNTY&quot; ## [11] &quot;CITY&quot; &quot;DAY&quot; &quot;MONTH&quot; &quot;YEAR&quot; &quot;DAY_WEEK&quot; ## [16] &quot;HOUR&quot; &quot;MINUTE&quot; &quot;NHS&quot; &quot;RUR_URB&quot; &quot;FUNC_SYS&quot; ## [21] &quot;RD_OWNER&quot; &quot;ROUTE&quot; &quot;TWAY_ID&quot; &quot;TWAY_ID2&quot; &quot;MILEPT&quot; ## [26] &quot;LATITUDE&quot; &quot;LONGITUD&quot; &quot;SP_JUR&quot; &quot;HARM_EV&quot; &quot;MAN_COLL&quot; ## [31] &quot;RELJCT1&quot; &quot;RELJCT2&quot; &quot;TYP_INT&quot; &quot;WRK_ZONE&quot; &quot;REL_ROAD&quot; ## [36] &quot;LGT_COND&quot; &quot;WEATHER1&quot; &quot;WEATHER2&quot; &quot;WEATHER&quot; &quot;SCH_BUS&quot; ## [41] &quot;RAIL&quot; &quot;NOT_HOUR&quot; &quot;NOT_MIN&quot; &quot;ARR_HOUR&quot; &quot;ARR_MIN&quot; ## [46] &quot;HOSP_HR&quot; &quot;HOSP_MN&quot; &quot;CF1&quot; &quot;CF2&quot; &quot;CF3&quot; ## [51] &quot;FATALS&quot; &quot;DRUNK_DR&quot; How to figure out what each variable means? Browse around the FARS server to see if you can find something that might help. I found a codebook here. (Just in case the FARS website changes, here’s a copy downloaded on 9/7/2016.) Let’s figure out what CF1 means. How about WEATHER2 and REL_LOAD? 3.2 Other tables But there are other data files in the database. dir(&quot;~/Downloads/FARS2015NationalDBF/&quot;) ## [1] &quot;ACC_AUX.dbf&quot; &quot;accident.dbf&quot; &quot;cevent.dbf&quot; &quot;Damage.dbf&quot; ## [5] &quot;Distract.dbf&quot; &quot;DrImpair.dbf&quot; &quot;Factor.dbf&quot; &quot;Maneuver.dbf&quot; ## [9] &quot;MIACC.dbf&quot; &quot;MIDRVACC.dbf&quot; &quot;MIPER.dbf&quot; &quot;nmcrash.dbf&quot; ## [13] &quot;NMImpair.dbf&quot; &quot;NMPrior.dbf&quot; &quot;parkwork.dbf&quot; &quot;PBType.dbf&quot; ## [17] &quot;PER_AUX.dbf&quot; &quot;person.dbf&quot; &quot;SafetyEq.dbf&quot; &quot;VEH_AUX.dbf&quot; ## [21] &quot;vehicle.dbf&quot; &quot;VEvent.dbf&quot; &quot;VINDecode.dbf&quot; &quot;Violatn.dbf&quot; ## [25] &quot;Vision.dbf&quot; &quot;VSOE.dbf&quot; Let’s look at some of them: head(Vision) ## STATE ST_CASE VEH_NO MVISOBSC ## 1 1 10001 1 0 ## 2 1 10002 1 0 ## 3 1 10003 1 0 ## 4 1 10004 1 0 ## 5 1 10005 1 2 ## 6 1 10005 2 0 head(Distract) ## STATE ST_CASE VEH_NO MDRDSTRD ## 1 1 10001 1 99 ## 2 1 10002 1 0 ## 3 1 10003 1 0 ## 4 1 10004 1 99 ## 5 1 10005 1 99 ## 6 1 10005 2 0 head(DrImpair) ## STATE ST_CASE VEH_NO DRIMPAIR ## 1 1 10001 1 0 ## 2 1 10002 1 0 ## 3 1 10003 1 9 ## 4 1 10004 1 9 ## 5 1 10005 1 98 ## 6 1 10005 2 0 What’s the connection between these tables and the Accidents table? Say, how would we be able to see which weather conditions distracted driving accidents tend to occur in? The call was cross-posted by the US Department of Transportation here↩ Such an address is called a URL.↩ "],
["case-study-taxicabs-and-the-sharing-economy.html", "§ 4 Case Study: Taxicabs and the sharing economy", " § 4 Case Study: Taxicabs and the sharing economy A team of mathematicians and engineers has calculated that if taxi riders were willing to share a cab, New York City could reduce the current fleet of 13,500 taxis up to 40 percent. Link to news story and an interactive site with the data. "],
["case-study-medicare-spending.html", "§ 5 Case Study: Medicare spending", " § 5 Case Study: Medicare spending Newspaper article here Data available here. DTK notes "],
["untidy-data-school-enrollments.html", "§ 6 Untidy data: School enrollments", " § 6 Untidy data: School enrollments The US Census Bureau collects data on many aspects of the population. Data on school enrollments is available here. We’re going to look at one of the data tables they make available: Table 2: Single Grade of Enrollment and High School Graduation Status for People 3 Years Old and Over, by Sex, Age (Single Years for 3 to 24 Years), Race, and Hispanic Origin: October 2014 XLS or CSV format. Download one of these files and open it in appropriate software. Or you can view the data on Google Drive here. How many people are represented in this data table? The table is in some ways a graphical visualization of features of school enrollment and age. (Unfocus your eyes and you will see a visual pattern.) What patterns do you see? The table indicates that 74.4% of the people in the table are “not enrolled” in school. Figure out how to calculate this from the numbers in the table. (Hint: You need only look at line 9.) These data are “untidy” in a technical sense. Identify the ways that they are untidy. Some columns contain information that can be calculated from other columns. Look at the data for 4-year olds and identify those that are calculated from other columns. Figure out the minimal set of columns from which the others could be calculated. Imagine that this table was created from a much bigger table in which each case is an individual person in the US. How many cases would there be in that table? What variables would you need so that you could calculate any entry in the “Table 2” provided by the Census Bureau? "],
["untidy-data-galtons-measurements-of-height.html", "§ 7 Untidy data: Galton’s measurements of height", " § 7 Untidy data: Galton’s measurements of height In the 1880s, Francis Galton started to make a mathematical theory of evolution. But the basic biology of heritability was not known: nothing about “genes” or DNA, etc. In order to create a theory, Galton needed a way to measure how traits are inherited from parents. To this end, he visited families in London and measured the heights of the parents and their (adult) children. Here’s part of a page from his lab notebook. A page from Francis Galton’s notebook. Divide into groups of 2 or 3 and translate the notebook data into a tidy form. Think about what would be an appropriate “case” for storing this data. What variables should there be? Open a spreadsheet and fill in a couple of rows of the tidy table that you envision. Here are a few that have already been made:Group-1, Group-2, Group-3, Group-4, Group-5, Group-6 "],
["untidy-data-family-structure-of-military-personnel.html", "§ 8 Untidy data: Family structure of military personnel", " § 8 Untidy data: Family structure of military personnel This spreadsheet contains is a presentation of data about family structure in the US Armed Forces. How many military personnel (not their children) are represented in this data table? What is a case in this data table? These data are “untidy” in a technical sense. Identify the ways that they are untidy. Some rows contain information that could be calculated from other rows. Identify these. Some “tabs” contain information that could be calculated from the other tabs. Identify these. Some columns contain information that can be calculated from other columns. Figure out the minimal set of columns from which the others could be calculated. Imagine that this table was created from a much bigger table in which each case is an individual person. How many cases would there be in that table? What variables would you need so that you could calculate any entry in the table linked to above. Divide into groups and fill in a few rows of the table you created. Here are a few that have already been made:Group-1, Group-2, Group-3, Group-4, Group-5, Group-6 "],
["untidy-data-minneapolis-voting.html", "§ 9 Untidy data: Minneapolis Voting", " § 9 Untidy data: Minneapolis Voting The spreadsheet here contains data on the Minneapolis 2013 election by ward and precinct. What is the case here? How are the data not tidy? What might these data look like in tidy form? The data table DataComputing::Minneapolis2013 lists the choices on individual ballots. What is the case? The cases in DataComputing::Minneapolis2013 can be aggregated to produce some of the variables in the spreadsheet. Which variables in the spreadsheet cannot be recreated from an aggregation of the ballot data? (Background on the voting law: to vote, a person must be registered in advance or do so at the polling place. Votes can be made at the polling place or, for a voter who is away, by mail as an absentee. Some ballots are not legible or otherwise violate voting rules; these are called “spoiled.” ) Imagine a data table like DataComputing::Minneapolis2013 that could be aggregated to produce the variables in the spreadsheet. What would the cases be in that table? "],
["tidy-data.html", "§ 10 Tidy Data 10.1 Data has all sorts of forms 10.2 Data Tables 10.3 Conversion from images, videos, etc. to data table 10.4 Cases and Variables 10.5 What’s a variable? 10.6 Not in Tidy Data 10.7 Cases 10.8 Basic Knowledge 10.9 Tidy Data 10.10 Workflow: Creating a chain of evidence 10.11 Summary", " § 10 Tidy Data 10.1 Data has all sorts of forms 10.1.1 Signals 10.1.2 Photographs 10.1.3 Video Follow this link! 10.1.4 Text, e.g. What am I doing? on OKCupid currently working as an international agent for a freight forwarding company. import, export, domestic you know the works online classes and trying to better myself in my free time. perhaps a hours worth of a good book or a video game on a lazy sunday.&quot; dedicating everyday to being an unbelievable badass. i make nerdy software for musicians, artists, and experimenters to indulge in their own weirdness, but i like to spend time away from the computer when working on my artwork (which is typically more concerned with group dynamics and communication, than with visual form, objects, or technology). i also record and deejay dance, noise, pop, and experimental music (most of which electronic or at least studio based). besides these relatively ego driven activities, i’ve been enjoying things like meditation and tai chi to try and gently flirt with ego death.&quot; reading things written by old dead people work work work work + play building awesome stuff. figuring out what’s important. having adventures. looking for treasure. digging up buried treasure 10.1.5 Sequences AMY1gene\" by Original uploader was TransControl at en.wikipedia - Transferred from en.wikipedia; transfer was stated to be made by en:User:Brandon5485.. Licensed under Public Domain via Wikimedia Commons. --> 10.2 Data Tables We’re going to use just one very simple format: the data table. ## name sex count year ## 1 Rotha F 7 1907 ## 2 Julian M 535 1948 ## 3 Christina M 22 1967 ## 4 Song F 11 1994 ## 5 Wayman M 9 1997 10.3 Conversion from images, videos, etc. to data table 10.3.1 OK Cupid Sentiment extraction 10.3.2 Tipi rings in Montana Assessment on family size based on tipi ring diameter Population size by adding up the rings 10.3.3 Animal tracking 10.4 Cases and Variables 10.4.1 Anatomy of a data table --> A row is always a case A column is always a variable 10.5 What’s a variable? A quantity or category that may vary from case to case. Two main types: Quantitative: a number Categorical: one of a set of discrete possibilities 10.6 Not in Tidy Data No units No footnotes Instead, this information should go into a codebook. Values and Cases need to be commensurate Same kind of thing for each case, e.g. don’t mix miles and km. Within a variable, only the same kind of value for each case. 10.7 Cases The object from which the variables were measured. Examples: A person, a country, an earthquake, a bike rental A person on a date A country in a year An earthquake and its aftershocks 10.8 Basic Knowledge What is each variable about. What is the kind of object that defines a case 10.9 Tidy Data Every value for each variable is the same kind of thing as all the other values for that variable. Every case is the same kind of thing as all the other cases. 10.10 Workflow: Creating a chain of evidence It’s important to be able to state definitely where your data came from. Part of this is not to edit your data. Once you have a table, don’t change anything in it. Instead, do your data-transformations in R so that you have a complete statement of how the data you collected are related to your analysis. 10.11 Summary Data Table: Rectangular format: cases (rows) and variables (columns) Separate analysis from data storage. Use a codebook to describe your cases and variables in detail Keep your data tidy "],
["r-programming-parts-of-speech.html", "§ 11 R Programming: Parts of Speech 11.1 Command chains 11.2 An example command chain 11.3 Syntax and semantics 11.4 Part of Speech 11.5 Parts of Speech in R 11.6 Data frames 11.7 Functions 11.8 Arguments 11.9 Variables 11.10 Constants 11.11 Discussion Problem", " § 11 R Programming: Parts of Speech 11.1 Command chains Your commands will be written as chains. Each link in the chain will be a data verb and its arguments. The very first link is usually a data frame. Links are connected by the chaining symbol %&gt;% Often, but not always, you will save the output of the chain in a named object. This is done with the assignment operator, &lt;- Name_of_result &lt;- Starting_data_frame %&gt;% first verb (arguments for details) %&gt;% next verb (and its arguments) %&gt;% ... and so on, up through ... last verb (and its arguments) 11.2 An example command chain Princes &lt;- BabyNames %&gt;% filter(grepl(&quot;Prince&quot;, name)) %&gt;% group_by(year) %&gt;% summarise(total = sum(count)) A good idea to put each link on its own line Note that %&gt;% is at the end of each line. Except … Princes &lt;- is assignment Except … The last line has no %&gt;%. 11.3 Syntax and semantics There are two distinct aspects involved in reading or writing a command chain. Syntax: the grammar of the command Semantics: the meaning of the command The focus today is on syntax. 11.4 Part of Speech From the dictionarty part of speech noun parts of speech a category to which a word is assigned in accordance with its syntactic functions. In English the main parts of speech are noun, pronoun, adjective, determiner, verb, adverb, preposition, conjunction, and interjection. 11.5 Parts of Speech in R Data frames Functions Arguments Variables Constants Assignment Formulas (we won’t use these until the end) 11.6 Data frames A data frame comprises one or more variables. Naming convention: data frames are given names that start with a CAPITAL LETTER, e.g., RegisteredVoters. A data frame will always be the input at the start of a command chain. If assignment is used to save the result, the object created is usually a data frame. 11.7 Functions Functions are objects that transform an input into an output. Functions are always followed by parentheses, that is, an opening ( and, eventually, a closing ). Each link in a command chain starts with a function. More specifically, the function is a data verb that takes a data frame as input and produces another data frame as output. There are other kinds of functions, e.g. summary (or reduction) functions and transformation functions. 11.8 Arguments The things that go inside a function’s parentheses are called arguments. Arguments describe the details of what a function is to do. If there are multiple arguments, they are always separated by commas. Many functions take named arguments which look like a name followed by an = sign, e.g. summarise(total = sum(count)) You can also consider the data frame passed along by %&gt;% as an argument to the following function. 11.9 Variables Variables are the components of data frames. When they are used, they always appear in function arguments, that is, between the function’s parentheses. A good convention is for variables to have names that start with a lower-case letter. The convention is not universally followed. Variables will never be followed by (. 11.10 Constants Constants are single values, most commonly a number or a character string. Character strings will always be in quotation marks, &quot;like this.&quot; Numerals are the written form of numbers, for instance. -42 1984 3.14159 11.11 Discussion Problem Consider this command chain: Princes &lt;- BabyNames %&gt;% filter(grepl(&quot;Prince&quot;, name)) %&gt;% group_by(year) %&gt;% summarise(total = sum(count)) Just from the syntax, you should be able to tell which of the five different kinds of object each of these things is: Princes, BabyNames, filter, grepl, &quot;Prince&quot;, name, group_by, year, summarise, total, sum, count. Explain your reasoning. "],
["part-data-summaries-and-graphics.html", "§ 12 (PART) Data Summaries and Graphics", " § 12 (PART) Data Summaries and Graphics "],
["data-vs-information.html", "§ 13 Data vs Information 13.1 The word “data”: 13.2 13.3 The word “information” 13.4 In this course 13.5 Data Reports 13.6 Glyph-ready Data", " § 13 Data vs Information What’s the difference. Although they are often used synonomously, in this course … Data is given; information is taken. 13.1 The word “data”: Dictionary etymology: 1640s, plural of datum, from Latin datum “(thing) given,” neuter past participle of dare “to give”. Historically: Data (Greek: Δεδομένα, Dedomena) is a work by Euclid. It deals with the nature and implications of “given” information in geometrical problems. The subject matter is closely related to the first four books of Euclid’s Elements. source 13.2 1838 English edition of Euclid 13.3 The word “information” The English word was apparently derived from the Latin stem (information-) of the nominative (informatio): this noun is derived from the verb informare (to inform) in the sense of “to give form to the mind”, “to discipline”, “instruct”, “teach”. Source: A Dictionary Definition: knowledge acquired through experience or study source 13.4 In this course Data are measurements, observations, records, etc. of the sort that appear in a data table. Information is the knowledge or belief that guides decisions or provides explanations. 13.4.1 Our Task Turn data into information. 13.5 Data Reports Knowledge and belief are attributes of the human mind. Knowledge is belief that there is consensus about. We derive knowledge and belief from our experiences including the statements of those we deem to have authority. A data report is a presentation of data from which we are inclined to draw conclusions. Some forms: A data table itself. It must be small to be assimilated by us. A data graphic. Good data graphics are those that display patterns (or the lack thereof) in a form that is readily assimulated and faithful to the data being reported. A data model, such as a statistical model written in a mathematical formalism. Data graphics can be interpreted without (much) specialized training. Models typically require some specialized skills interpretation. 13.6 Glyph-ready Data Making data graphics can be straightforward so long as the data underlying the graphic are in the appropriate form. We’ll call such a form glyph-ready. Data Wrangling is the process of transforming or reshaping the data tables that arrive at our door into a glyph-ready form. The glyph-ready form depends on the kind of graphic you wish to make. Data reports in general also use data in glyph-ready form, so wrangling techniques useful for graphics are also useful for data modeling or the construction of short, human-interpretable tables. "],
["glyphs-frames-and-scales.html", "§ 14 Glyphs, Frames, and Scales 14.1 Glyphs and Data 14.2 Data Glyph 14.3 Data Glyph Properties: Aesthetics 14.4 Why “Aesthetic”? 14.5 Some Graphics Components 14.6 Scales 14.7 Guides 14.8 Facets – using x and y twice 14.9 Designing Graphics 14.10 Good and Bad Graphics 14.11 Perception and Comparison 14.12 Count the ways this graphic is bad 14.13 Glyph-Ready Data 14.14 Layers – building up complex plots 14.15 Stats: Data Transformations 14.16 What’s Next", " § 14 Glyphs, Frames, and Scales 14.1 Glyphs and Data In its original sense, in archeology, a glyph is a carved symbol. Heiroglyph Mayan glyph 14.2 Data Glyph 14.2.1 A data glyph is also a mark, e.g. The features of a data glyph encodes the value of variables. Some are very simple, e.g. a dot: Some combine different elements, e.g. a pointrange: Some are complicated, e.g. a dotplot: See: http://docs.ggplot2.org/current/ 14.3 Data Glyph Properties: Aesthetics Aesthetics are visual properties of a glyph. Aesthetics for points: location (x and y), shape, color, size, transparency ## Warning: Using size for a discrete variable is not advised. Each glyph has its own set of aesthetics. 14.4 Why “Aesthetic”? 14.5 Some Graphics Components glyph The basic graphical unit that represents one case. Other terms used include mark and symbol. aesthetic a visual property of a glyph such as position, size, shape, color, etc. may be mapped based on data values: sex -&gt; color may be set to particular non-data related values: color is black scale A mapping that translates data values into aesthetics. example: male -&gt; blue; female -&gt; pink frame The position scale describing how data are mapped to x and y guide An indication for the human viewer of the scale. This allows the viewer to translate aesthetics back into data values. Examples: x- and y-axes, various sorts of legends 14.6 Scales The relationship between the variable value and the value of the aesthetic the variable is mapped to. Systolic Blood Pressure (SBP) has units of mmHg (millimeters of mercury) Position on the x-axis measured in distance, e.g. inches. The conversion from SBP to position is a scale. Smoker is “never”, “former”, “current” Color is red, green, blue, … The conversion from Smoker to color is a scale. 14.7 Guides Guide: an indication to a human viewer of what the scale is. Example: Axis ticks and numbers Legends . . Labels on faceted graphics 14.8 Facets – using x and y twice x is determined by sbp and sex basically a separate frame for each sex 14.9 Designing Graphics Graphics are designed by the human expert (you!) in order to reveal information that’s latent in the data. 14.9.0.1 Design choices What kind of glyph, e.g. scatter, density, bar, … many others What variables constitute the frame. And some details: axis limits logarithmic axes, etc. What variables should be mapped to other aesthetics of the glyph. Whether to facet and with what variable. More details, …, e.g. setting of aesthetics to constants 14.10 Good and Bad Graphics Remember … Graphics are designed by the human expert (you!) in order to reveal information that’s latent in the data. Your choices depend on what information you want to reveal and convey. Learn by reading graphics and determining which ways of arranging thing are better or worse. A basic principle is that a graphic is about comparison. Good graphics make it easy for people to perceive things that are similar and things that are different. Good graphics put the things to be compared “side-by-side”, that is, in perceptual proximity to one another. 14.11 Perception and Comparison In roughly descending order of human ability to compare nearby objects: Position Length Area Angle Shape (but only a very few different shapes) Color Color is the most difficult, because it is a 3-dimensional quantity. - color gradients — we’re good at - discrete colors — must be carefully selected. 14.12 Count the ways this graphic is bad ## Warning: Using size for a discrete variable is not advised. 14.13 Glyph-Ready Data Glyph-ready data has this form: There is one row for each glyph to be drawn. The variables in that row are mapped to aesthetics of the glyph (including position) Glyph-ready data ## sbp dbp sex smoker ## 1 129 75 male never ## 2 105 62 female never ## 3 122 72 male never ## 4 128 83 female former ## 5 123 90 male former ## 6 122 77 male current Mapping of data to aesthetics sbp -&gt; x dbp -&gt; y smoker -&gt; color sex -&gt; shape Scales determine details of data -&gt; aesthetic translation 14.14 Layers – building up complex plots Each layer may have its own data, glyphs, aesthetic mapping, etc. one layer has points another layer has the curves 14.15 Stats: Data Transformations What are the glyphs, aesthetics, etc. for this plot? How is the data for this plot related to the “raw” data? ## sbp dbp sex smoker ## 1 129 75 male never ## 2 105 62 female never ## 3 122 72 male never ## 4 128 83 female former 14.16 What’s Next Eye-training recognize and describe glyphs, aesthetics, scales, etc. identify data required for a plot Data wrangling get data into glyph-ready format (dplyr, tidyr) Graphics construction start with: map variables to aesthetics interactively with scatterGraphHelper(), barGraphHelper(), densityGraphHelper() move on to: describe data, glyphs, aesthetics, etc. to R using ggplot2 "],
["activity-mapping-the-stars.html", "§ 15 Activity: Mapping the stars", " § 15 Activity: Mapping the stars Figure 15.1: Stars plotted on the celestial sphere by the Gaia space telescope. (Sept. 2016) The image above is a map of the stars constructed by the European Space Agency’s Gaia space telescope. It reportedly shows 1,000,000,000 stars.3 Although the map represents one billion stars, the image itself is only 660 by 398 pixels: a total of 262,680 pixels altogether. How can a billion stars be displayed in only one-quarter of a million pixels? Why is the image oval? Why are there broad, curving bands of shading? How might these reflect layers of the graphic that display different quantities? (See the codebook for some ideas about variables that might reflect the available data rather than the stars themselves.) Gaia data are available in CSV form at this site. A codebook is here Download one of the CSV files and see what you can make of it. For instance, … You can see the .csv in the name. What does the .gz mean at the end of the file name? How many stars are there in this one file? From the number of such .csv.gz files available, estimate how many stars there are in the complete catalog. Make a map of the stars in your one file. (Suggestion: in developing your plot, just use several thousand stars from the file. Otherwise things will be slow. Select the stars at random.) Use phot_g_mean_flux as the intensity and ecl_lon and ecl_lat as the position variables. Explore a bit and decide what are good aesthetics for representing the intensity. (Hints: color? size?) Does faceting make sense? A simple plot: 7. Is there a relationship between the ra and dec variables and the ecl_lon and ecl_lat variables? Try different ways assigning the variables to aesthetics until you find one that tells the story. Optional: Requires some mathematical sophistication. Make a conformal-map style presentation of the relationship between the ra/dec coordinate system and the ecl_lat/ecl_lon system. Suggestion: Pull out only those stars that fall within a narrow band of the edges of a square in one of the coordinate systems. Then make separate plots of those stars in the two systems, perhaps using color to encode which stars in one plot correspond to stars in the other plot. See this story on the BBC web site.↩ "],
["introduction-to-graphics-and-wrangling.html", "§ 16 Introduction to Graphics and Wrangling 16.1 Deconstructing graphics 16.2 Wrangling", " § 16 Introduction to Graphics and Wrangling PDF handout In today’s activity, you are going to deconstruct some graphics and carry out some data wrangling operations. 16.1 Deconstructing graphics Figure 16.1: A representation of some of the variables from the data table in the package. Figure 16.2: Variables from the data table in the package. The yes' andno’ refers to whether the person is pregnant. Considering each of the above graphics in turn, figure out: What mode of graphic is it? (e.g. density plot, scatter plot, bar plot, …) What variables from the respective data tables are involved? What role each of those variables plays in the graphic? In Figure 2, why is there no data variable being used for the \\(y\\)-axis? Here is the basic structure of the commands for making the graphics. You can try various combinations of the variables appearing in the graphics and see which graphic you think is the most informative. ggplot(data = CPS85, aes(x = ????, y = ????, color = ????)) + geom_point() + facet_wrap( ~ ????) ggplot(data = NCHS, aes(x = ????)) + geom_density(aes(color = ????)) + facet_wrap(~ ???) Put the R statement that generates each graph into your report so that the graphs appear when you compile your .Rmd file. 16.2 Wrangling Diamonds Refer to the diamonds data table in the ggplot2 package. Take a look at the codebook (using help()) so that you’ll understand the meaning of the tasks. (Motivated by Garrett Grolemund.) Each of the following tasks can be accomplished by a statement of the form For each task, give appropriate R functions or arguments to substitute in place of verb1, verb2, args1, args2, and args3. Which color diamonds seem to be largest on average (in terms of carats)? Which clarity of diamonds has the largest average “table” per carat? Voting Using the Minneapolis2013 data table, answer these questions: How many cases are there? Who were the top 5 candidates in the Second vote selections. How many ballots are marked “undervote” in First choice selections? Second choice selections? Third choice selections? What are the top 3 Second vote selections among people who voted for Betsy Hodges as their first choice? Which Precinct had the highest fraction of First vote selections marked as “undervote”? "],
["part-data-verbs.html", "§ 17 (PART) Data Verbs", " § 17 (PART) Data Verbs "],
["basicdataverbs.html", "§ 18 Basic Data Verbs 18.1 Three kinds of verbs for data wrangling 18.2 Data Verbs 18.3 Reduction verbs 18.4 Transformation verbs", " § 18 Basic Data Verbs 18.1 Three kinds of verbs for data wrangling Data verbs Reduction verbs Transformation verbs 18.1.1 Non-wrangling verbs There will also be verbs for graphics, loading data, etc., but for wrangling we’ll need mainly these three types. Examples: library() – attaches the software distributed in a package to your session of R read.csv() and other file-reading functions. Creates a data table given the location of a file containing those data. scatterGraphHelper() – takes a data table as input, but produces graphics as output. data() – accesses data from a package. data() is not a data verb! 18.2 Data Verbs What distinquishes a data verb from a reduction or transformation verb? Data verbs create a new data table, from an input data table. 18.2.1 Some commonly used data verbs. summarise() group_by() filter() mutate() select() arrange() join (there’s a family of joins – more on this later) 18.3 Reduction verbs Characteristics? Variable in, a single number out. Examples? 18.4 Transformation verbs Variable in, variable out. Examples? "],
["more-transformation-verbs.html", "§ 19 More transformation verbs 19.1 Rank transforms 19.2 Leads and lags 19.3 Times and Dates 19.4 Text", " § 19 More transformation verbs Recall that a transformation verb takes a variable as an input and produces a variable of the same length as the output. Some familiar transformation verbs from primary and secondary school: arithmetic operations (+, -, /, *) mathematical functions such as logs and exponentiation There are additional transformations which will be unfamiliar, simply because they did not fit into the algebra- and trigonometry-based high-school curriculum Rank transforms. Lead and lag transforms. Date transforms Character transforms. 19.1 Rank transforms Many questions take forms such as these: “Find the largest …” “Find the three largest …” “Find the smallest within each group …” The functions min() and max() are reduction verbs. They tell you the single lowest or highest value in a set. Because they are reduction verbs, they are often used in summarise(), which reduces a set of cases to a single case. ## # A tibble: 2 × 2 ## sex most_popular ## &lt;chr&gt; &lt;int&gt; ## 1 F 99674 ## 2 M 94758 Notice that name was not carried along. When you summarise(), the only variables that appear in the output are the grouping variables, and the variables you create through the arguments to summarise(). If you wanted to know the names that are most popular, you will need to rely on filter(). Get rid of the cases that are not the most popular. ## Source: local data frame [2 x 4] ## Groups: sex [2] ## ## name sex count year ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; ## 1 Linda F 99674 1947 ## 2 James M 94758 1947 Filter() needs a criterion. The criterion count == max( count ) (with the double equals sign ==) passes through the case where the value of count matches the largest value of count. That will be the biggest case. ## Source: local data frame [28 x 4] ## Groups: sex [2] ## ## name sex count year ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; ## 1 James M 87428 1946 ## 2 Linda F 99674 1947 ## 3 James M 94758 1947 ## 4 Robert M 91652 1947 ## 5 John M 88318 1947 ## 6 Linda F 96210 1948 ## 7 James M 88610 1948 ## 8 Robert M 85492 1948 ## 9 Linda F 90994 1949 ## 10 James M 86779 1949 ## # ... with 18 more rows Note: Almost all of these are from the late 1940s and early 1950s. In part, this reflects the baby boom. Perhaps it also reflects the conformity that people associate with that era. QUESTION: How would you estimate “conformity” for each year? Possibility: Take the 10 most popular names each year. Find out what fraction of the total number of births that was. The rank() operation is helpful here. The rank() function does something simple but powerful: it replaces each number in a set with where that number stands with respect to the others. For instance, look at the tiny data table Set shown in Table . What’s the rank of the number 5 in the numbers variable. ## numbers the_rank ## 1 2 1.5 ## 2 5 4.0 ## 3 4 3.0 ## 4 7 5.0 ## 5 2 1.5 ## 6 9 7.5 ## 7 9 7.5 ## 8 8 6.0 Or, seen another way ## numbers the_rank ## 1 2 1.5 ## 2 2 1.5 ## 3 4 3.0 ## 4 5 4.0 ## 5 7 5.0 ## 6 8 6.0 ## 7 9 7.5 ## 8 9 7.5 Notice how ties are broken. Also note that the biggest numbers have the highest ranks. This is different than the convention in everyday language, where the “Number 1 ranked team” is the best team. To follow this convention, use rank(desc(numbers)). ## numbers the_rank ## 1 2 7.5 ## 2 2 7.5 ## 3 4 6.0 ## 4 5 5.0 ## 5 7 4.0 ## 6 8 3.0 ## 7 9 1.5 ## 8 9 1.5 Suppose you want to find the 3rd most popular name of all time. Use rank(). ## # A tibble: 1 × 2 ## name total ## &lt;chr&gt; &lt;int&gt; ## 1 Robert 4809858 Or, to find the top three most popular names, replace == in the above by &lt;=. ## # A tibble: 3 × 2 ## name total ## &lt;chr&gt; &lt;int&gt; ## 1 James 5114325 ## 2 John 5095590 ## 3 Robert 4809858 When applied to grouped data, rank() will be calculated separately within each group. That is, the rank of a value will be with respect to the other cases in that group. For instance, here’s the third most popular name for each sex. ## Source: local data frame [6 x 4] ## Groups: sex [2] ## ## name sex count year ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; ## 1 Linda F 99674 1947 ## 2 James M 94758 1947 ## 3 Robert M 91652 1947 ## 4 Linda F 96210 1948 ## 5 Linda F 90994 1949 ## 6 Michael M 92711 1957 19.1.1 Tied ranks Sometimes, two or more numbers are tied in rank. The rank() function deals with these by assigning all the tied values the same rank, which is the mean of the ranks those values would have had if they were even slightly different. There are other rank-like transformation verbs that handle ties differently. For instance, row_number() breaks ties in favor of the first case encountered. ## numbers the_rank ties_broken ## 1 2 1.5 1 ## 2 2 1.5 2 ## 3 4 3.0 3 ## 4 5 4.0 4 ## 5 7 5.0 5 ## 6 8 6.0 6 ## 7 9 7.5 7 ## 8 9 7.5 8 19.2 Leads and lags ## numbers next_one ## 1 2 NA ## 2 5 2 ## 3 4 5 ## 4 7 4 ## 5 2 7 ## 6 9 2 ## 7 9 9 ## 8 8 9 Find the names that increase the most from one year to the next. Let’s take “increase the most” to mean “the biggest proportional increase”, but consider only names that have more than 100 kids in the earlier year. Find proportional increase for each name for each year, but push this to zero if the base year had less than 100 kids. ## name sex count year ## 1 Hillary F 5 1922 ## 2 Hillary F 7 1942 ## 3 Hillary F 13 1943 ## 4 Hillary F 19 1944 ## 5 Hillary F 19 1945 ## 6 Hillary F 29 1946 ## 7 Hillary F 51 1947 ## 8 Hillary F 43 1948 ## 9 Hillary F 48 1949 ## 10 Hillary F 54 1950 ## 11 Hillary F 40 1951 ## 12 Hillary F 43 1952 ## 13 Hillary F 57 1953 ## 14 Hillary F 78 1954 ## 15 Hillary F 71 1955 ## 16 Hillary F 100 1956 ## 17 Hillary F 87 1957 ## 18 Hillary F 94 1958 ## 19 Hillary F 83 1959 ## 20 Hillary F 75 1960 ## 21 Hillary F 111 1961 ## 22 Hillary F 112 1962 ## 23 Hillary F 148 1963 ## 24 Hillary F 132 1964 ## 25 Hillary F 111 1965 ## 26 Hillary F 123 1966 ## 27 Hillary F 155 1967 ## 28 Hillary F 178 1968 ## 29 Hillary F 198 1969 ## 30 Hillary F 258 1970 ## 31 Hillary F 251 1971 ## 32 Hillary F 242 1972 ## 33 Hillary F 250 1973 ## 34 Hillary F 313 1974 ## 35 Hillary F 323 1975 ## 36 Hillary F 331 1976 ## 37 Hillary F 442 1977 ## 38 Hillary F 718 1978 ## 39 Hillary F 921 1979 ## 40 Hillary F 835 1980 ## 41 Hillary F 736 1981 ## 42 Hillary F 752 1982 ## 43 Hillary F 696 1983 ## 44 Hillary F 1009 1984 ## 45 Hillary F 1047 1985 ## 46 Hillary F 1082 1986 ## 47 Hillary F 1080 1987 ## 48 Hillary F 1083 1988 ## 49 Hillary F 1285 1989 ## 50 Hillary F 1524 1990 ## 51 Hillary F 1788 1991 ## 52 Hillary F 2522 1992 ## 53 Hillary F 1064 1993 ## 54 Hillary F 409 1994 ## 55 Hillary F 310 1995 ## 56 Hillary F 312 1996 ## 57 Hillary F 294 1997 ## 58 Hillary F 243 1998 ## 59 Hillary F 253 1999 ## 60 Hillary F 252 2000 ## 61 Hillary F 259 2001 ## 62 Hillary F 197 2002 ## 63 Hillary F 224 2003 ## 64 Hillary F 310 2004 ## 65 Hillary F 285 2005 ## 66 Hillary F 265 2006 ## 67 Hillary F 282 2007 ## 68 Hillary F 412 2008 ## 69 Hillary F 200 2009 ## 70 Hillary F 167 2010 ## 71 Hillary F 166 2011 ## 72 Hillary F 157 2012 ## 73 Hillary F 154 2013 Notice “Woodrow” and “Wilson” in 1911 to 1912. Why? Notice “Samantha” and “Darin” in 1964. Why? (Hint: “Bewitched”) FLAW: There might be years left out for some names. We’ll have to wait until we study joins to see how to fix this. 19.3 Times and Dates With the lubridate package: Transform text dates into an R type with numerical properties. ymd(), dmy(), and so on. Extract parts of the date: day() jday() week() hour() wday() minute() month() year() ## [1] &quot;character&quot; ## [1] &quot;Date&quot; ## middle ## 1 1999-11-26 ## [1] &quot;2016-09-28 22:30:29 CDT&quot; 19.4 Text Simple operators: tolower() toupper() `nchar() For later: gsub() grepl() DataComputing::extractMatches() "],
["trends-in-popularity-of-names.html", "§ 20 Trends in Popularity of Names 20.1 Objective", " § 20 Trends in Popularity of Names The relative popularity of different names for babies varies over the years and decades. Let’s construct a visualization of how the popularity of names varies in time. 20.1 Objective Create a graph like the following using name of interest to you. The raw material you have is the BabyNames data set in the DataComputing package. Point out which variables are categorical. These can potentially be used for defining groups of cases. 20.1.1 First, Individually … 20.1.1.1 Step 1. Analyze the graphic to figure out what a glyph-ready data table should look like. Mostly, this involves figuring out what variables are represented in the graph. Write down a small example of a glyph-ready data frame that you think could be used to make something in the form of the graphic. What variable(s) from the raw data table do not appear at all in the graph? What variable(s) in the graph are similar to corresponding variables in the raw data table, but might have been transformed in some way. 20.1.1.2 Step 2 Consider how the cases differ between the raw input and the glyph-ready table. Have cases been filtered out? Have cases been grouped and aggregated/summarized within groups in any way? Have any new variables been introduced? 20.1.1.3 Step 3 Using English, write down a sequence of steps that will accomplish the transfiguration from the raw data table to your hypothesized glyph-ready data table. 20.1.1.4 Step 4: Confer with your colleagues As a group, compare your different analyses in Steps 1 through 3. Your goal is to develop a consensus for the design in Step 3. 20.1.1.5 Step 5: Implementation Now you can start writing the commands themselves. Do so, try to identify and solve any problems that arise, and make your glyph-ready data. For graphing, you can use this template: "],
["case-study-in-basic-data-verbs-moby-dick.html", "§ 21 Case study in basic data verbs: Moby Dick 21.1 Prolog: Scraping and arranging the data 21.2 Most common words 21.3 Most common sequences", " § 21 Case study in basic data verbs: Moby Dick 21.1 Prolog: Scraping and arranging the data A text file of the book is available at http://www.gutenberg.org/ebooks/2701. At that page is a link to a UTF-8 encoded text document named &quot;pg2701.txt&quot;. I downloaded the file and stored it on my machine as Data/pg2701.txt. I can read that using readLines(). Moby &lt;- readLines(&quot;Data/pg2701.txt&quot;) You could also read the file directly from Project Gutenberg con &lt;- file(&quot;http://www.gutenberg.org/ebooks/2701.txt.utf-8&quot;) Moby &lt;- readLines(con) close(con) The result, stored in the Moby object, is a character vector of 22108 strings. Some of these are prefatory matter, some postscript. The text itself begins after a line start_text &lt;- &quot;START OF THIS PROJECT GUTENBERG&quot; and ends before a line end_text &lt;- &quot;END OF THIS PROJECT GUTENBERG&quot; Using these as delimiters includes some transcriber’s notes, etc. For simplicity, I’ll take as the start the line start_text &lt;- &quot;CHAPTER 1\\\\.&quot; The last line is simply “orphan.” ending line end_text &lt;- &quot;^orphan\\\\.$&quot; Why the funny spelling? The start_text and end_text are being specified as a “regex” (sometimes called regular expression) indicating that the word “orphan” is at the very beginning of the line, followed by a period and the end of the line. Regexes are a way of describing patterns. For our purposes, we’ll use them to identify the first and last line of Melville’s work in the Project Gutenberg text. As it happens, there are two instances of “CHAPTER 1.” in Moby Dick. The second is a book within the book. We want to start with the early instance. first_line &lt;- min(grep(start_text, Moby)) last_line &lt;- grep(end_text, Moby) Moby &lt;- Moby[first_line : last_line] We want to break the strings up into individual words. We’ll do this “by hand” because I want to render the text as a simple set of words and punctuation. Steps: Change punctuation so that it is an isolated character. Split up each line by spaces into words. Convert to lower case (because I’m not interested in capitalization). tmp &lt;- Moby characters &lt;- unlist(strsplit(tolower(Moby), split = NULL)) # Step 1 punctuation &lt;- c(&quot;.&quot;, &quot;,&quot;, &quot;;&quot;, &quot;:&quot;, &quot;?&quot;, &quot;!&quot;, &#39;&quot;&#39;, &quot;&#39;&quot;, &quot;&amp;&quot;, &quot;-&quot;, &quot;(&quot;, &quot;)&quot;, &quot;[&quot;, &quot;]&quot;) for (symbol in punctuation) { result &lt;- paste0(&quot; &quot;, symbol, &quot; &quot;) tmp &lt;- gsub(symbol, result, tmp, fixed=TRUE ) } # Step 2 Words &lt;- unlist(strsplit(tmp, split = &quot; &quot;)) # Step 3 Words &lt;- data.frame(word = tolower(Words), stringsAsFactors = FALSE) # Get rid of empty strings # Words &lt;- # Words %&gt;% # filter(word != &quot;&quot;) What are the character frequencies in the book? table(characters) %&gt;% data.frame(stringsAsFactors = FALSE) %&gt;% arrange(desc(Freq)) ## characters Freq ## 1 190500 ## 2 e 115021 ## 3 t 86551 ## 4 a 76496 ## 5 o 68135 ## 6 n 64555 ## 7 i 64384 ## 8 s 63107 ## 9 h 61779 ## 10 r 51160 ## 11 l 42049 ## 12 d 37658 ## 13 u 26316 ## 14 m 22904 ## 15 c 22143 ## 16 w 21774 ## 17 g 20493 ## 18 f 20476 ## 19 , 18947 ## 20 p 16961 ## 21 y 16602 ## 22 b 16600 ## 23 v 8418 ## 24 k 7937 ## 25 . 7385 ## 26 - 5741 ## 27 ; 4143 ## 28 &quot; 2879 ## 29 &#39; 2850 ## 30 ! 1741 ## 31 q 1544 ## 32 j 1061 ## 33 x 1006 ## 34 ? 999 ## 35 z 623 ## 36 ( 201 ## 37 ) 201 ## 38 : 192 ## 39 0 123 ## 40 1 123 ## 41 2 54 ## 42 5 52 ## 43 7 47 ## 44 8 47 ## 45 * 45 ## 46 3 45 ## 47 4 34 ## 48 6 31 ## 49 9 31 ## 50 _ 4 ## 51 [ 2 ## 52 ] 2 ## 53 &amp; 2 ## 54 $ 2 21.2 Most common words Popular &lt;- Words %&gt;% group_by(word) %&gt;% tally() %&gt;% arrange(desc(n)) %&gt;% head(50) 21.3 Most common sequences Sequences &lt;- Words %&gt;% filter(grepl(&quot;[a-zA-Z]&quot;, word)) %&gt;% mutate(two = lead(word, 1), three = lead(two, 1), four = lead(three, 1)) CommonPairs &lt;- Sequences %&gt;% group_by(word, two) %&gt;% tally() %&gt;% ungroup() %&gt;% arrange(desc(n)) Popular triplets Triplets &lt;- Sequences %&gt;% group_by(word, two, three) %&gt;% tally() %&gt;% ungroup() %&gt;% arrange(desc(n)) "],
["cities-of-the-world.html", "§ 22 Cities of the World", " § 22 Cities of the World The data table WorldCities (in the DCF package) identifies cities around the world that have large populations or are large for their region. Check the table for plausibility: is it possibly what it is claimed to be?. For instance … What’s the total number of people represented? Explain why or why not the data pass this plausibility test. Create another plausibility test and describe it. It can be very simple. If you can, implement it and state whether the data table passes the test. How many cities larger than 100,000? Larger than 1,000,000? Make a scatterplot of the latitude and longitude of cities larger than 100K. Decide what variables to map to the \\(x\\) and \\(y\\) aesthetics.4 Use the size of the dot to show the city’s population. In other words, map the variable population to the size aesthetic. Use transparency, called alpha, to handle overplotting. Alpha can run from zero to one: zero is completely transparent (a.k.a. invisible); one is completely opaque. You will be setting alpha the same for every city. Recall that in ggplot graphics, variables are mapped to aesthetics using the aes() function. In contrast, aesthetic properties that are the same for every case are set outside the aes() function. In a typical use, the ggplot() command will look like ggplot( data=???, aes( x=???, y=??? )) The layers of the plot will be used like this: geom_point( alpha=???, aes( size=??? ) ) where, of course, you will replace the ??? with appropriate variables or constants or data tables. When you have your plotting commands complete, use those commands to make another graphic, but add this expression to govern the size attribute: + scale_size_area(). This will make the area of the dot proportional to the value of the variable mapped to it. Without scale_size_area(), the diameter of the dot is proportional to the variable. Explain which scale, area or diameter, you think is most informative. (Include both graphics in your Rmd file along with your explanation.) Create a data table BiggestByCountry that has the one biggest city in each country. Plot the locations of BiggestByCountry as another layer in your graphic. Make them red. Add to the graphic the names of the cities from BiggestByCountry. Hint: use geom_text(). Set the size=2. Remember, setting is different from mapping a variable. You’ll use the label= aesthetic to represent the city names. Find the countries where the biggest city is more than 5M people The resulting table will have a couple of dozen cases. Display as output in your report all the cases but just these variables: city name, country, and population. Remember, “aesthetic” is being used in its original sense: how things are perceived.↩ "],
["additional-exercises-on-data-verbs.html", "§ 23 Additional Exercises on Data Verbs 23.1 How many births", " § 23 Additional Exercises on Data Verbs 23.1 How many births You might think that you can find the number of babies born each year in the US by using the Social Security BabyNames data. Just group by year and add up the counts: But some babies are missing from this tally. In particular, the BabyNames data only reports names, years, and sex for which there were five or more births. Presumably there were some babies whose names were so unusual that they are unique, or shared by only two, three, or four babies. Your job, make an estimate of how many such babies there are. For simplicity, make that estimate for a single year, say 2010. Here’s an approach. Count the total number of -names with 5, 6, … 19, 20 births. That is, consider all the names of each sex with only 5 babies reported and find the total number of babies that fall into that class. The same for names-and-sex with 6 births, and so on. See if you can spot a pattern with how the number of babies changes depends on each count.Then extrapolate out to 1, 2, 3, 4 and use that to estimate the total population. The pattern is very consistent across the different levels of “numbers of names”. It looks like we’re missing about 100,000 babies in 2010. "],
["stocks-and-dividends.html", "§ 24 Stocks and Dividends 24.1 Task 24.2 Getting Price Data 24.3 Buy/Sell Profit 24.4 Indexing Prices 24.5 Dividends", " § 24 Stocks and Dividends Many companies are publicly traded. This means that the company issues stock certificates which can be bought and sold on an exchange. Investors buy stock certificates mainly because they can sell them in the future, perhaps making a profit, and because companies pay dividends, a share of the company’s profit, to each shareholder. Figure 24.1: Stock prices for Ford, Honda, and Toyota Figure 24.2: Accumulated dividends (per share) paid by Ford, Honda, and Toyota Figures and show the scale of price fluctuations and of accumulated dividends. 24.1 Task Compare the income (or loss) that comes from buying and selling a stock certificate to the income that comes from dividends over the same period. Answer these questions: Which source of income is bigger? Is there any correlation between the income from dividends and the income (or loss) from buying and selling? 24.2 Getting Price Data Sites such as finance.yahoo.com collect and distribute information about individual companies. You can use the readStockPrices() function (in the DataComputing package) to read such data directly from Yahoo into R. For example, here are some automotive stocks and their daily prices from 2010 to 2015. Choose a few companies of interest to you. You can find stock company symbols at finance.yahoo.com. (Suggestion: pick a sector of the economy, e.g. energy, high-tech, consumer products, etc. and use companies from that sector.) Plot out the “closing price” (Close) versus date to get a graphic like Figure . 24.3 Buy/Sell Profit Pick a buy date and a sell date. You can use a command like this to create a Table like that shown in Table . Combine Prices and Actions to produce a table like SalesDifference in Table : Hints: (1) What kind of join should you use so that you get only those cases that match one of the dates in the Actions table? (2) The wide-vs-long techniques in Chapter will be useful. From the data table with buy and sell prices, calculate the dollar amount of profit (or loss) and the percentage change, as in Table . 24.4 Indexing Prices Since stock prices vary markedly from one company to another, a common practice is to “index” the price to a particular date as in Figure . (Question: In the graph, roughly which date was used for the reference?) Pick a single date of your choice and extract the stock price information for each company on that date. In the result, there should be one case for each company. Select just the date, company, and close variables, renaming close as standard. Call the resulting data frame Reference. Figure 24.3: Indexed stock prices for Ford, Honda, and Toyota You now need to combine the Reference with each day’s price data for that company. You’ll find the standardized price on each day by creating a new variable which is the ratio of the day-to-day price (use Close) to the standard for that company. Before you can do this, you’ll need to combine the Prices and Reference data tables. You’ll use a join verb to do this. In order to check your results, sketch out what you think the result should be before you do the join. 24.5 Dividends You can read in dividend data like this: Once you have the dividend data, extract out the dividends for all dates between your buy and sell dates. (Hint: Join Dividends to Actions using company to match. The result will have two. When ) The dividend amount is actually a rate: the dividend paid (in dollars) divided by the stock price. Find the dollar amount of each dividend payment for one share of stock rather than one dollar of stock. This involves multiplying the dividend rate by the stock price on that date. Find the total amount of dividends for each company during the period of interest. Compare this amount to the profit (or loss) from buying and selling the stock certificates. For the car companies, the result for the period 2005-01-01 though 2014-12-31 is shown in Table . "],
["bicycle-use-patterns.html", "§ 25 Bicycle use patterns 25.1 Time of day", " § 25 Bicycle use patterns PDF handout In this activity, you’ll examine some factors that may influence the use of bicycles in a bike-renting program. The data come from Washington, DC and cover the last quarter of 2014. We will use the “Trips-history” data file You can access the data like this. Important: To avoid repeatedly re-reading the files, start the above chunk with rather than the usual . The Trips data table is a random subset of 10,000 trips from the full quarterly data. Start with this small data table to develop your analysis commands. When you have this working well, you can access the full data set of more than 600,000 events by removing -Small from the name of the data_site. 25.1 Time of day It’s natural to expect that bikes are rented more at some times of day than others. The variable sdate gives the time (including the date) that the rental started. Make these plots and interpret them: A density plot of the events versus sdate. Use ggplot() and geom_density(). A density plot of the events versus time of day. You can use lubridate::hour(), and lubridate::minute() to extract the hour of the day and minute within the hour from sdate, e.g. Trips %&gt;% mutate(time_of_day = lubridate::hour(sdate) + lubridate::minute(sdate) / 60) %&gt;% ... further processing ... Facet (2) by day of the week. (Use lubridate::wday() to generate day of the week. Use + facet_wrap( ~ day_of_week) to perform the faceting.) Set the fill aesthetic for geom_density() to the client variable.5 You may also want to set the alpha for transparency and color=NA to suppress the outline of the density function. Same as (4) but using geom_density() with the argument position = position_stack(). In your opinion, which of these graphics is most effective at telling an interesting story? Rather than faceting on day of the week, consider creating a new faceting variable like this: mutate(wday = ifelse(lubridate::wday(sdate) %in% c(1,7), &quot;weekend&quot;, &quot;weekday&quot;)) Is it better to facet on wday and fill with client, or vice versa? client describes whether the renter is a regular user (level Registered) or has not joined the bike-rental organization (Causal).↩ "],
["births-and-holidays.html", "§ 26 Births and Holidays 26.1 Births and holidays", " § 26 Births and Holidays PDF handout The number of daily births in the US varies over the year and from day to day. What’s surprising to many people is that the variation from one day to the next can be huge: some days have only about 80% as many births as others. Why? The data table Birthdays in the mosaicData package gives the number of births recorded on each day of the year in each state from 1969 to 1988. (It would be nice to have more recent data, but I don’t have them at hand.) For this activity, we’ll work with data aggregated across the states. Create a new data table, DailyBirths, that adds up all the births for each day across all the states. Plot out daily births vs date. The date variable in Birthdays prints out in the conventional, human-readable way. But it is actually in a format (called POSIX date format) that automatically respects the order of time. The lubridate package contains helpful functions that will extract various information about any date. Here are some you might find useful: year() month() week() yday() — gives the day of the year as a number 1-366. This is often called the “Julian day.” mday() — gives the day of the month as a number 1-31 wday() — gives the weekday (e.g. Monday, Tuesday, …). Use the optional argument label=TRUE to have the weekday spelled out rather than given as a number 1-7. Using these lubridate functions, you can easily look at the data in more detail. To examine seasonality in birth rates, look at the number of births aggregated over all the years by each week each month each Julian day To examine patterns within the week, look at the number of births by day of the week. Pick a two-year span of the Birthdays that falls in the 1980s, say, 1980/1981. Extract out the data just in this interval, calling it MyTwoYears. (Hint: filter(), year()). Plot out the births in this two-year span day by day. Color each date according to its day of the week. Explain the pattern that you see. 26.1 Births and holidays A few days each year don’t follow the pattern in (4). We’re going to examine the hypothesis that these are holidays. You can find a data set listing US federal holidays at http://tiny.cc/dcf/US-Holidays.csv. Read it in as follows:6 Add a couple of layers to your plot from (4). Draw a vertical bar at each date which is a holiday. You’ll use the geom_vline() glyph. You can give a data = argument to geom_vline() to tell it to plot out the information from Holidays rather than MyTwoYears.7 Add a text label to each of the vertical bars to identify which holiday it is. Use the geom_text() glyph.8 The plot in (6) is too busy. Let’s explore some other ways to display the data to make it clearer to the view that holidays tend to be low-birth days. Add a variable to MyTwoYears called is_holiday. It should be TRUE when the day is a holiday, and FALSE otherwise. One way to do this is with the transformation verb %in%, for instance, is_holiday = date %in% Holidays$date Make a new plot where you map is_holiday to the shape or the size aesthetics, or perhaps for faceting. Or perhaps change the way color is used in the graph to show weekends and holidays separately from non-holiday weekdays. Make the graph as simple as you can until you get one that clearly tells the story. You may want to simplify by eliminating any components of the graph (e.g. holiday labels? dots? lines? vertical lines?) that aren’t essential to telling the story. The point of the lubridate::dmy() function is to convert the character-string date stored in the CSV to a POSIX date-number.↩ Unfortunately, due to what I believe is a bug in geom_vline(), you will have to set the x position of the bars by as.numeric(date) rather than just date().↩ Hints: You’ll have to make up a y-coordinate for each label. You can set the orientation of each label with the angle aesthetic.↩ "],
["part-joins.html", "§ 27 (PART) Joins", " § 27 (PART) Joins "],
["biblical-names.html", "§ 28 Biblical Names", " § 28 Biblical Names A list of Bible-related names is available this way: Using this data table and BabyNames: Make a data table showing the most popular biblenames over all the years. Make an informative plot showing the trends over the years of Bible-related names as a proportion of all names. "],
["conformity-and-names.html", "§ 29 Conformity and Names", " § 29 Conformity and Names Calculate the total number of babies given the 10 most popular names each year as a fraction of the total number of babies born that year. Question: What would have happened if group_by() came after filter()? We’ll need an operation to combine two tables to find the proportion each year. Here’s a teaser: "],
["some-resources.html", "§ 30 Some resources", " § 30 Some resources Data scraping: https://github.com/ropensci/user2016-tutorial Text analysis of tweets: http://varianceexplained.org/r/trump-tweets/ "]
]
