[
["gettingorganized.html", "Notes and Materials for Data Computing § 1 (PART) Getting Organized", " Notes and Materials for Data Computing Daniel Kaplan 2016-09-21 § 1 (PART) Getting Organized The materials from Week 1 will go here. Getting to RStudio Connecting to GitHub RMarkdown Writing a simple RMarkdown document "],
["datainfrastructure.html", "§ 2 (PART) Data Infrastructure", " § 2 (PART) Data Infrastructure "],
["a-bogus-chapter-title.html", "§ 3 A Bogus Chapter title Topics", " § 3 A Bogus Chapter title Topics The structure of tabular data cases and variables numerical and categorical variables tidy data R Commands Files and documents "],
["case-study-highway-fatalities.html", "§ 4 Case Study: Highway Fatalities 4.1 The accident data 4.2 Other tables", " § 4 Case Study: Highway Fatalities On August 29, 2016, the White House issued a data-science “call to action.”1(https://www.transportation.gov/fastlane/2015-traffic-fatalities-data-has-just-been-released-call-action-download-and-analyze). Today, the U.S. Department of Transportation is releasing an open data set that contains detailed, anonymized information about each of these tragic incidents. As the new data being released show, and as DOT reported earlier this summer, 2015 showed a marked increase in traffic fatalities nationwide. To be precise, 7.2% more people died in traffic-related accidents in 2015 than in 2014. This unfortunate data point breaks a recent historical trend of fewer deaths occurring per year. Under the leadership of Transportation Secretary Anthony Foxx, we’re doing two things differently this year. One: We’re publishing the data through NHTSA’s Fatality Analysis Reporting System (FARS) three months earlier than last year. Two: We’re directly soliciting your help to better understand what these data are telling us. Whether you’re a non-profit, a tech company, or just a curious citizen wanting to contribute to the conversation in your local community, we want you to jump in and help us understand what the data are telling us. Some key questions worth exploring: How might improving economic conditions around the country change how Americans are getting around? What models can we develop to identify communities that might be at a higher risk for fatal crashes? How might climate change increase the risk of fatal crashes in a community? How might we use studies of attitudes toward speeding, distracted driving, and seat belt use to better target marketing and behavioral change campaigns? How might we monitor public health indicators and behavior risk indicators to target communities that might have a high prevalence of behaviors linked with fatal crashes (drinking, drug use/addiction, etc.)? DOT is aggressively seeking ways to improve safety on the roads. From our work with the auto industry to improve vehicle safety, to new solutions to behavioral challenges like drunk, drugged, distracted and drowsy driving, we know we need to find novel solutions to old challenges. We’re also looking to accelerate technologies that may make driving safer, including connected and highly automated vehicles. But we need your help, too! Data Science is a team sport. We are calling on data scientists, public health experts, students and researchers—even if you have never thought about road safety before—to dive in to these data and help answer these important questions, especially on tough issues like pedestrian and bicyclist fatalities. Start by downloading and playing with the data. Then share your insights and let us know what you find by sending us a note at opendata@dot.gov. 4.1 The accident data The link to the data in the call to action is ftp://ftp.nhtsa.dot.gov/fars/2015/.2 Go to that site. Is it immediately clear what’s going on? What can you figure out by browsing the site. look in “parent” directories try substituting 2015 for other similar sorts of values in the URL. A blog by Lucas Puente conveniently gives simple instructions for downloading the data. He writes: Simply visit ftp://ftp.nhtsa.dot.gov/fars/2015/National/ and download the FARS2015NationalDBF.zip file, unzip it, and load into R. After unzipping, there is a directory FARS2015NationalDBF taking up 874.9 MB of disk for 27 items. I put it in my Downloads directory. Lucas provides commands to read the data into R. library(foreign) accidents &lt;- read.dbf(&quot;FARS2015NationalDBF/accident.dbf&quot;) To make sense of these instructions, it helps to know some things: What is library(foreign) doing? What does the library() part of the command tell you? What is foreign. How would get get some instructions or documentation for foreign to help you understand why this is appropriate. What is read.dbf()? What does &quot;FARS2015NationalDBF/accident.dbf&quot; tell you about the data file and where it’s located. Aside: I would rather you wrote: filename &lt;- &quot;~/Downloads/FARS2015NationalDBF/accident.dbf&quot; Accidents &lt;- foreign::read.dbf(filename) What’s different about the file name I’m using? Why? Lucas’s blog leads you through the steps of making a map of accident locations: Lucas’s map What does this map tell you? With the Accidents data table read into R, it’s easy to look at it and perhaps construct summaries. ## STATE ST_CASE VE_TOTAL VE_FORMS PVH_INVL PEDS PERNOTMVIT PERMVIT ## 1 1 10001 1 1 0 0 0 1 ## 2 1 10002 1 1 0 0 0 1 ## 3 1 10003 1 1 0 0 0 2 ## 4 1 10004 1 1 0 0 0 1 Some simple things: nrow(Accidents) ## [1] 32166 names(Accidents) ## [1] &quot;STATE&quot; &quot;ST_CASE&quot; &quot;VE_TOTAL&quot; &quot;VE_FORMS&quot; &quot;PVH_INVL&quot; ## [6] &quot;PEDS&quot; &quot;PERNOTMVIT&quot; &quot;PERMVIT&quot; &quot;PERSONS&quot; &quot;COUNTY&quot; ## [11] &quot;CITY&quot; &quot;DAY&quot; &quot;MONTH&quot; &quot;YEAR&quot; &quot;DAY_WEEK&quot; ## [16] &quot;HOUR&quot; &quot;MINUTE&quot; &quot;NHS&quot; &quot;RUR_URB&quot; &quot;FUNC_SYS&quot; ## [21] &quot;RD_OWNER&quot; &quot;ROUTE&quot; &quot;TWAY_ID&quot; &quot;TWAY_ID2&quot; &quot;MILEPT&quot; ## [26] &quot;LATITUDE&quot; &quot;LONGITUD&quot; &quot;SP_JUR&quot; &quot;HARM_EV&quot; &quot;MAN_COLL&quot; ## [31] &quot;RELJCT1&quot; &quot;RELJCT2&quot; &quot;TYP_INT&quot; &quot;WRK_ZONE&quot; &quot;REL_ROAD&quot; ## [36] &quot;LGT_COND&quot; &quot;WEATHER1&quot; &quot;WEATHER2&quot; &quot;WEATHER&quot; &quot;SCH_BUS&quot; ## [41] &quot;RAIL&quot; &quot;NOT_HOUR&quot; &quot;NOT_MIN&quot; &quot;ARR_HOUR&quot; &quot;ARR_MIN&quot; ## [46] &quot;HOSP_HR&quot; &quot;HOSP_MN&quot; &quot;CF1&quot; &quot;CF2&quot; &quot;CF3&quot; ## [51] &quot;FATALS&quot; &quot;DRUNK_DR&quot; How to figure out what each variable means? Browse around the FARS server to see if you can find something that might help. I found a codebook here. (Just in case the FARS website changes, here’s a copy downloaded on 9/7/2016.) Let’s figure out what CF1 means. How about WEATHER2 and REL_LOAD? 4.2 Other tables But there are other data files in the database. dir(&quot;~/Downloads/FARS2015NationalDBF/&quot;) ## [1] &quot;ACC_AUX.dbf&quot; &quot;accident.dbf&quot; &quot;cevent.dbf&quot; &quot;Damage.dbf&quot; ## [5] &quot;Distract.dbf&quot; &quot;DrImpair.dbf&quot; &quot;Factor.dbf&quot; &quot;Maneuver.dbf&quot; ## [9] &quot;MIACC.dbf&quot; &quot;MIDRVACC.dbf&quot; &quot;MIPER.dbf&quot; &quot;nmcrash.dbf&quot; ## [13] &quot;NMImpair.dbf&quot; &quot;NMPrior.dbf&quot; &quot;parkwork.dbf&quot; &quot;PBType.dbf&quot; ## [17] &quot;PER_AUX.dbf&quot; &quot;person.dbf&quot; &quot;SafetyEq.dbf&quot; &quot;VEH_AUX.dbf&quot; ## [21] &quot;vehicle.dbf&quot; &quot;VEvent.dbf&quot; &quot;VINDecode.dbf&quot; &quot;Violatn.dbf&quot; ## [25] &quot;Vision.dbf&quot; &quot;VSOE.dbf&quot; Let’s look at some of them: head(Vision) ## STATE ST_CASE VEH_NO MVISOBSC ## 1 1 10001 1 0 ## 2 1 10002 1 0 ## 3 1 10003 1 0 ## 4 1 10004 1 0 ## 5 1 10005 1 2 ## 6 1 10005 2 0 head(Distract) ## STATE ST_CASE VEH_NO MDRDSTRD ## 1 1 10001 1 99 ## 2 1 10002 1 0 ## 3 1 10003 1 0 ## 4 1 10004 1 99 ## 5 1 10005 1 99 ## 6 1 10005 2 0 head(DrImpair) ## STATE ST_CASE VEH_NO DRIMPAIR ## 1 1 10001 1 0 ## 2 1 10002 1 0 ## 3 1 10003 1 9 ## 4 1 10004 1 9 ## 5 1 10005 1 98 ## 6 1 10005 2 0 What’s the connection between these tables and the Accidents table? Say, how would we be able to see which weather conditions distracted driving accidents tend to occur in? The call was cross-posted by the US Department of Transportation here↩ Such an address is called a URL.↩ "],
["case-study-taxicabs-and-the-sharing-economy.html", "§ 5 Case Study: Taxicabs and the sharing economy", " § 5 Case Study: Taxicabs and the sharing economy A team of mathematicians and engineers has calculated that if taxi riders were willing to share a cab, New York City could reduce the current fleet of 13,500 taxis up to 40 percent. Link to news story and an interactive site with the data. "],
["case-study-medicare-spending.html", "§ 6 Case Study: Medicare spending", " § 6 Case Study: Medicare spending Newspaper article here Data available here. DTK notes "],
["untidy-data-school-enrollments.html", "§ 7 Untidy data: School enrollments", " § 7 Untidy data: School enrollments The US Census Bureau collects data on many aspects of the population. Data on school enrollments is available here. We’re going to look at one of the data tables they make available: Table 2: Single Grade of Enrollment and High School Graduation Status for People 3 Years Old and Over, by Sex, Age (Single Years for 3 to 24 Years), Race, and Hispanic Origin: October 2014 XLS or CSV format. Download one of these files and open it in appropriate software. Or you can view the data on Google Drive here. How many people are represented in this data table? The table is in some ways a graphical visualization of features of school enrollment and age. (Unfocus your eyes and you will see a visual pattern.) What patterns do you see? The table indicates that 74.4% of the people in the table are “not enrolled” in school. Figure out how to calculate this from the numbers in the table. (Hint: You need only look at line 9.) These data are “untidy” in a technical sense. Identify the ways that they are untidy. Some columns contain information that can be calculated from other columns. Look at the data for 4-year olds and identify those that are calculated from other columns. Figure out the minimal set of columns from which the others could be calculated. Imagine that this table was created from a much bigger table in which each case is an individual person in the US. How many cases would there be in that table? What variables would you need so that you could calculate any entry in the “Table 2” provided by the Census Bureau? "],
["untidy-data-galtons-measurements-of-height.html", "§ 8 Untidy data: Galton’s measurements of height", " § 8 Untidy data: Galton’s measurements of height In the 1880s, Francis Galton started to make a mathematical theory of evolution. But the basic biology of heritability was not known: nothing about “genes” or DNA, etc. In order to create a theory, Galton needed a way to measure how traits are inherited from parents. To this end, he visited families in London and measured the heights of the parents and their (adult) children. Here’s part of a page from his lab notebook. A page from Francis Galton’s notebook. Divide into groups of 2 or 3 and translate the notebook data into a tidy form. Think about what would be an appropriate “case” for storing this data. What variables should there be? Open a spreadsheet and fill in a couple of rows of the tidy table that you envision. Here are a few that have already been made:Group-1, Group-2, Group-3, Group-4, Group-5, Group-6 "],
["untidy-data-family-structure-of-military-personnel.html", "§ 9 Untidy data: Family structure of military personnel", " § 9 Untidy data: Family structure of military personnel This spreadsheet contains is a presentation of data about family structure in the US Armed Forces. How many military personnel (not their children) are represented in this data table? What is a case in this data table? These data are “untidy” in a technical sense. Identify the ways that they are untidy. Some rows contain information that could be calculated from other rows. Identify these. Some “tabs” contain information that could be calculated from the other tabs. Identify these. Some columns contain information that can be calculated from other columns. Figure out the minimal set of columns from which the others could be calculated. Imagine that this table was created from a much bigger table in which each case is an individual person. How many cases would there be in that table? What variables would you need so that you could calculate any entry in the table linked to above. Divide into groups and fill in a few rows of the table you created. Here are a few that have already been made:Group-1, Group-2, Group-3, Group-4, Group-5, Group-6 "],
["untidy-data-minneapolis-voting.html", "§ 10 Untidy data: Minneapolis Voting", " § 10 Untidy data: Minneapolis Voting The spreadsheet here contains data on the Minneapolis 2013 election by ward and precinct. What is the case here? How are the data not tidy? What might these data look like in tidy form? The data table DataComputing::Minneapolis2013 lists the choices on individual ballots. What is the case? The cases in DataComputing::Minneapolis2013 can be aggregated to produce some of the variables in the spreadsheet. Which variables in the spreadsheet cannot be recreated from an aggregation of the ballot data? (Background on the voting law: to vote, a person must be registered in advance or do so at the polling place. Votes can be made at the polling place or, for a voter who is away, by mail as an absentee. Some ballots are not legible or otherwise violate voting rules; these are called “spoiled.” ) Imagine a data table like DataComputing::Minneapolis2013 that could be aggregated to produce the variables in the spreadsheet. What would the cases be in that table? "],
["tidy-data.html", "§ 11 Tidy Data 11.1 Data has all sorts of forms 11.2 Data Tables 11.3 Conversion from images, videos, etc. to data table 11.4 Cases and Variables 11.5 What’s a variable? 11.6 Not in Tidy Data 11.7 Cases 11.8 Basic Knowledge 11.9 Tidy Data 11.10 Workflow: Creating a chain of evidence 11.11 Summary", " § 11 Tidy Data 11.1 Data has all sorts of forms 11.1.1 Signals 11.1.2 Photographs 11.1.3 Video Follow this link! 11.1.4 Text, e.g. What am I doing? on OKCupid currently working as an international agent for a freight forwarding company. import, export, domestic you know the works online classes and trying to better myself in my free time. perhaps a hours worth of a good book or a video game on a lazy sunday.&quot; dedicating everyday to being an unbelievable badass. i make nerdy software for musicians, artists, and experimenters to indulge in their own weirdness, but i like to spend time away from the computer when working on my artwork (which is typically more concerned with group dynamics and communication, than with visual form, objects, or technology). i also record and deejay dance, noise, pop, and experimental music (most of which electronic or at least studio based). besides these relatively ego driven activities, i’ve been enjoying things like meditation and tai chi to try and gently flirt with ego death.&quot; reading things written by old dead people work work work work + play building awesome stuff. figuring out what’s important. having adventures. looking for treasure. digging up buried treasure 11.1.5 Sequences AMY1gene\" by Original uploader was TransControl at en.wikipedia - Transferred from en.wikipedia; transfer was stated to be made by en:User:Brandon5485.. Licensed under Public Domain via Wikimedia Commons. --> 11.2 Data Tables We’re going to use just one very simple format: the data table. ## name sex count year ## 1 Rotha F 7 1907 ## 2 Julian M 535 1948 ## 3 Christina M 22 1967 ## 4 Song F 11 1994 ## 5 Wayman M 9 1997 11.3 Conversion from images, videos, etc. to data table 11.3.1 OK Cupid Sentiment extraction 11.3.2 Tipi rings in Montana Assessment on family size based on tipi ring diameter Population size by adding up the rings 11.3.3 Animal tracking 11.4 Cases and Variables 11.4.1 Anatomy of a data table --> A row is always a case A column is always a variable 11.5 What’s a variable? A quantity or category that may vary from case to case. Two main types: Quantitative: a number Categorical: one of a set of discrete possibilities 11.6 Not in Tidy Data No units No footnotes Instead, this information should go into a codebook. Values and Cases need to be commensurate Same kind of thing for each case, e.g. don’t mix miles and km. Within a variable, only the same kind of value for each case. 11.7 Cases The object from which the variables were measured. Examples: A person, a country, an earthquake, a bike rental A person on a date A country in a year An earthquake and its aftershocks 11.8 Basic Knowledge What is each variable about. What is the kind of object that defines a case 11.9 Tidy Data Every value for each variable is the same kind of thing as all the other values for that variable. Every case is the same kind of thing as all the other cases. 11.10 Workflow: Creating a chain of evidence It’s important to be able to state definitely where your data came from. Part of this is not to edit your data. Once you have a table, don’t change anything in it. Instead, do your data-transformations in R so that you have a complete statement of how the data you collected are related to your analysis. 11.11 Summary Data Table: Rectangular format: cases (rows) and variables (columns) Separate analysis from data storage. Use a codebook to describe your cases and variables in detail Keep your data tidy "],
["r-programming-parts-of-speech.html", "§ 12 R Programming: Parts of Speech 12.1 Command chains 12.2 An example command chain 12.3 Syntax and semantics 12.4 Part of Speech 12.5 Parts of Speech in R 12.6 Data frames 12.7 Functions 12.8 Arguments 12.9 Variables 12.10 Constants 12.11 Discussion Problem", " § 12 R Programming: Parts of Speech 12.1 Command chains Your commands will be written as chains. Each link in the chain will be a data verb and its arguments. The very first link is usually a data frame. Links are connected by the chaining symbol %&gt;% Often, but not always, you will save the output of the chain in a named object. This is done with the assignment operator, &lt;- Name_of_result &lt;- Starting_data_frame %&gt;% first verb (arguments for details) %&gt;% next verb (and its arguments) %&gt;% ... and so on, up through ... last verb (and its arguments) 12.2 An example command chain Princes &lt;- BabyNames %&gt;% filter(grepl(&quot;Prince&quot;, name)) %&gt;% group_by(year) %&gt;% summarise(total = sum(count)) A good idea to put each link on its own line Note that %&gt;% is at the end of each line. Except … Princes &lt;- is assignment Except … The last line has no %&gt;%. 12.3 Syntax and semantics There are two distinct aspects involved in reading or writing a command chain. Syntax: the grammar of the command Semantics: the meaning of the command The focus today is on syntax. 12.4 Part of Speech From the dictionarty part of speech noun parts of speech a category to which a word is assigned in accordance with its syntactic functions. In English the main parts of speech are noun, pronoun, adjective, determiner, verb, adverb, preposition, conjunction, and interjection. 12.5 Parts of Speech in R Data frames Functions Arguments Variables Constants Assignment Formulas (we won’t use these until the end) 12.6 Data frames A data frame comprises one or more variables. Naming convention: data frames are given names that start with a CAPITAL LETTER, e.g., RegisteredVoters. A data frame will always be the input at the start of a command chain. If assignment is used to save the result, the object created is usually a data frame. 12.7 Functions Functions are objects that transform an input into an output. Functions are always followed by parentheses, that is, an opening ( and, eventually, a closing ). Each link in a command chain starts with a function. More specifically, the function is a data verb that takes a data frame as input and produces another data frame as output. There are other kinds of functions, e.g. summary (or reduction) functions and transformation functions. 12.8 Arguments The things that go inside a function’s parentheses are called arguments. Arguments describe the details of what a function is to do. If there are multiple arguments, they are always separated by commas. Many functions take named arguments which look like a name followed by an = sign, e.g. summarise(total = sum(count)) You can also consider the data frame passed along by %&gt;% as an argument to the following function. 12.9 Variables Variables are the components of data frames. When they are used, they always appear in function arguments, that is, between the function’s parentheses. A good convention is for variables to have names that start with a lower-case letter. The convention is not universally followed. Variables will never be followed by (. 12.10 Constants Constants are single values, most commonly a number or a character string. Character strings will always be in quotation marks, &quot;like this.&quot; Numerals are the written form of numbers, for instance. -42 1984 3.14159 12.11 Discussion Problem Consider this command chain: Princes &lt;- BabyNames %&gt;% filter(grepl(&quot;Prince&quot;, name)) %&gt;% group_by(year) %&gt;% summarise(total = sum(count)) Just from the syntax, you should be able to tell which of the five different kinds of object each of these things is: Princes, BabyNames, filter, grepl, &quot;Prince&quot;, name, group_by, year, summarise, total, sum, count. Explain your reasoning. "],
["part-data-summaries-and-graphics.html", "§ 13 (PART) Data Summaries and Graphics", " § 13 (PART) Data Summaries and Graphics "],
["data-vs-information.html", "§ 14 Data vs Information 14.1 The word “data”: 14.2 14.3 The word “information” 14.4 In this course 14.5 Data Reports 14.6 Glyph-ready Data", " § 14 Data vs Information What’s the difference. Although they are often used synonomously, in this course … Data is given; information is taken. 14.1 The word “data”: Dictionary etymology: 1640s, plural of datum, from Latin datum “(thing) given,” neuter past participle of dare “to give”. Historically: Data (Greek: Δεδομένα, Dedomena) is a work by Euclid. It deals with the nature and implications of “given” information in geometrical problems. The subject matter is closely related to the first four books of Euclid’s Elements. source 14.2 1838 English edition of Euclid 14.3 The word “information” The English word was apparently derived from the Latin stem (information-) of the nominative (informatio): this noun is derived from the verb informare (to inform) in the sense of “to give form to the mind”, “to discipline”, “instruct”, “teach”. Source: A Dictionary Definition: knowledge acquired through experience or study source 14.4 In this course Data are measurements, observations, records, etc. of the sort that appear in a data table. Information is the knowledge or belief that guides decisions or provides explanations. 14.4.1 Our Task Turn data into information. 14.5 Data Reports Knowledge and belief are attributes of the human mind. Knowledge is belief that there is consensus about. We derive knowledge and belief from our experiences including the statements of those we deem to have authority. A data report is a presentation of data from which we are inclined to draw conclusions. Some forms: A data table itself. It must be small to be assimilated by us. A data graphic. Good data graphics are those that display patterns (or the lack thereof) in a form that is readily assimulated and faithful to the data being reported. A data model, such as a statistical model written in a mathematical formalism. Data graphics can be interpreted without (much) specialized training. Models typically require some specialized skills interpretation. 14.6 Glyph-ready Data Making data graphics can be straightforward so long as the data underlying the graphic are in the appropriate form. We’ll call such a form glyph-ready. Data Wrangling is the process of transforming or reshaping the data tables that arrive at our door into a glyph-ready form. The glyph-ready form depends on the kind of graphic you wish to make. Data reports in general also use data in glyph-ready form, so wrangling techniques useful for graphics are also useful for data modeling or the construction of short, human-interpretable tables. "],
["basicdataverbs.html", "§ 15 Basic Data Verbs 15.1 Three kinds of verbs for data wrangling 15.2 Data Verbs 15.3 Reduction verbs 15.4 Transformation verbs", " § 15 Basic Data Verbs 15.1 Three kinds of verbs for data wrangling Data verbs Reduction verbs Transformation verbs 15.1.1 Non-wrangling verbs There will also be verbs for graphics, loading data, etc., but for wrangling we’ll need mainly these three types. Examples: library() – attaches the software distributed in a package to your session of R read.csv() and other file-reading functions. Creates a data table given the location of a file containing those data. scatterGraphHelper() – takes a data table as input, but produces graphics as output. data() – accesses data from a package. data() is not a data verb! 15.2 Data Verbs What distinquishes a data verb from a reduction or transformation verb? Data verbs create a new data table, from an input data table. 15.2.1 Some commonly used data verbs. summarise() group_by() filter() mutate() select() arrange() join (there’s a family of joins – more on this later) 15.3 Reduction verbs Characteristics? Variable in, a single number out. Examples? 15.4 Transformation verbs Variable in, variable out. Examples? "],
["glyphs-frames-and-scales.html", "§ 16 Glyphs, Frames, and Scales 16.1 Glyphs and Data 16.2 Data Glyph 16.3 Data Glyph Properties: Aesthetics 16.4 Why “Aesthetic”? 16.5 Some Graphics Components 16.6 Scales 16.7 Guides 16.8 Facets – using x and y twice 16.9 Designing Graphics 16.10 Good and Bad Graphics 16.11 Perception and Comparison 16.12 Count the ways this graphic is bad 16.13 Glyph-Ready Data 16.14 Layers – building up complex plots 16.15 Stats: Data Transformations 16.16 What’s Next", " § 16 Glyphs, Frames, and Scales 16.1 Glyphs and Data In its original sense, in archeology, a glyph is a carved symbol. Heiroglyph Mayan glyph 16.2 Data Glyph 16.2.1 A data glyph is also a mark, e.g. The features of a data glyph encodes the value of variables. Some are very simple, e.g. a dot: Some combine different elements, e.g. a pointrange: Some are complicated, e.g. a dotplot: See: http://docs.ggplot2.org/current/ 16.3 Data Glyph Properties: Aesthetics Aesthetics are visual properties of a glyph. Aesthetics for points: location (x and y), shape, color, size, transparency ## Warning: Using size for a discrete variable is not advised. Each glyph has its own set of aesthetics. 16.4 Why “Aesthetic”? 16.5 Some Graphics Components glyph The basic graphical unit that represents one case. Other terms used include mark and symbol. aesthetic a visual property of a glyph such as position, size, shape, color, etc. may be mapped based on data values: sex -&gt; color may be set to particular non-data related values: color is black scale A mapping that translates data values into aesthetics. example: male -&gt; blue; female -&gt; pink frame The position scale describing how data are mapped to x and y guide An indication for the human viewer of the scale. This allows the viewer to translate aesthetics back into data values. Examples: x- and y-axes, various sorts of legends 16.6 Scales The relationship between the variable value and the value of the aesthetic the variable is mapped to. Systolic Blood Pressure (SBP) has units of mmHg (millimeters of mercury) Position on the x-axis measured in distance, e.g. inches. The conversion from SBP to position is a scale. Smoker is “never”, “former”, “current” Color is red, green, blue, … The conversion from Smoker to color is a scale. 16.7 Guides Guide: an indication to a human viewer of what the scale is. Example: Axis ticks and numbers Legends . . Labels on faceted graphics 16.8 Facets – using x and y twice x is determined by sbp and sex basically a separate frame for each sex 16.9 Designing Graphics Graphics are designed by the human expert (you!) in order to reveal information that’s latent in the data. 16.9.0.1 Design choices What kind of glyph, e.g. scatter, density, bar, … many others What variables constitute the frame. And some details: axis limits logarithmic axes, etc. What variables should be mapped to other aesthetics of the glyph. Whether to facet and with what variable. More details, …, e.g. setting of aesthetics to constants 16.10 Good and Bad Graphics Remember … Graphics are designed by the human expert (you!) in order to reveal information that’s latent in the data. Your choices depend on what information you want to reveal and convey. Learn by reading graphics and determining which ways of arranging thing are better or worse. A basic principle is that a graphic is about comparison. Good graphics make it easy for people to perceive things that are similar and things that are different. Good graphics put the things to be compared “side-by-side”, that is, in perceptual proximity to one another. 16.11 Perception and Comparison In roughly descending order of human ability to compare nearby objects: Position Length Area Angle Shape (but only a very few different shapes) Color Color is the most difficult, because it is a 3-dimensional quantity. - color gradients — we’re good at - discrete colors — must be carefully selected. 16.12 Count the ways this graphic is bad ## Warning: Using size for a discrete variable is not advised. 16.13 Glyph-Ready Data Glyph-ready data has this form: There is one row for each glyph to be drawn. The variables in that row are mapped to aesthetics of the glyph (including position) Glyph-ready data ## sbp dbp sex smoker ## 1 129 75 male never ## 2 105 62 female never ## 3 122 72 male never ## 4 128 83 female former ## 5 123 90 male former ## 6 122 77 male current Mapping of data to aesthetics sbp -&gt; x dbp -&gt; y smoker -&gt; color sex -&gt; shape Scales determine details of data -&gt; aesthetic translation 16.14 Layers – building up complex plots Each layer may have its own data, glyphs, aesthetic mapping, etc. one layer has points another layer has the curves 16.15 Stats: Data Transformations What are the glyphs, aesthetics, etc. for this plot? How is the data for this plot related to the “raw” data? ## sbp dbp sex smoker ## 1 129 75 male never ## 2 105 62 female never ## 3 122 72 male never ## 4 128 83 female former 16.16 What’s Next Eye-training recognize and describe glyphs, aesthetics, scales, etc. identify data required for a plot Data wrangling get data into glyph-ready format (dplyr, tidyr) Graphics construction start with: map variables to aesthetics interactively with scatterGraphHelper(), barGraphHelper(), densityGraphHelper() move on to: describe data, glyphs, aesthetics, etc. to R using ggplot2 "],
["activity-mapping-the-stars.html", "§ 17 Activity: Mapping the stars", " § 17 Activity: Mapping the stars Figure 17.1: Stars plotted on the celestial sphere by the Gaia space telescope. (Sept. 2016) The image above is a map of the stars constructed by the European Space Agency’s Gaia space telescope. It reportedly shows 1,000,000,000 stars.3 Although the map represents one billion stars, the image itself is only 660 by 398 pixels: a total of 262,680 pixels altogether. How can a billion stars be displayed in only one-quarter of a million pixels? Why is the image oval? Why are there broad, curving bands of shading? How might these reflect layers of the graphic that display different quantities? (See the codebook for some ideas about variables that might reflect the available data rather than the stars themselves.) Gaia data are available in CSV form at this site. A codebook is here Download one of the CSV files and see what you can make of it. For instance, … You can see the .csv in the name. What does the .gz mean at the end of the file name? How many stars are there in this one file? From the number of such .csv.gz files available, estimate how many stars there are in the complete catalog. Make a map of the stars in your one file. (Suggestion: in developing your plot, just use several thousand stars from the file. Otherwise things will be slow. Select the stars at random.) Use phot_g_mean_flux as the intensity and ecl_lon and ecl_lat as the position variables. Explore a bit and decide what are good aesthetics for representing the intensity. (Hints: color? size?) Does faceting make sense? A simple plot: 7. Is there a relationship between the ra and dec variables and the ecl_lon and ecl_lat variables? Try different ways assigning the variables to aesthetics until you find one that tells the story. Optional: Requires some mathematical sophistication. Make a conformal-map style presentation of the relationship between the ra/dec coordinate system and the ecl_lat/ecl_lon system. Suggestion: Pull out only those stars that fall within a narrow band of the edges of a square in one of the coordinate systems. Then make separate plots of those stars in the two systems, perhaps using color to encode which stars in one plot correspond to stars in the other plot. See this story on the BBC web site.↩ "],
["case-study-in-basic-data-verbs-moby-dick.html", "§ 18 Case study in basic data verbs: Moby Dick 18.1 Prolog: Scraping and arranging the data 18.2 Most common words 18.3 Most common sequences", " § 18 Case study in basic data verbs: Moby Dick 18.1 Prolog: Scraping and arranging the data A text file of the book is available at http://www.gutenberg.org/ebooks/2701. At that page is a link to a UTF-8 encoded text document named &quot;pg2701.txt&quot;. I downloaded the file and stored it on my machine as Data/pg2701.txt. I can read that using readLines(). Moby &lt;- readLines(&quot;Data/pg2701.txt&quot;) You could also read the file directly from Project Gutenberg con &lt;- file(&quot;http://www.gutenberg.org/ebooks/2701.txt.utf-8&quot;) Moby &lt;- readLines(con) close(con) The result, stored in the Moby object, is a character vector of 22108 strings. Some of these are prefatory matter, some postscript. The text itself begins after a line start_text &lt;- &quot;START OF THIS PROJECT GUTENBERG&quot; and ends before a line end_text &lt;- &quot;END OF THIS PROJECT GUTENBERG&quot; Using these as delimiters includes some transcriber’s notes, etc. For simplicity, I’ll take as the start the line start_text &lt;- &quot;CHAPTER 1\\\\.&quot; The last line is simply “orphan.” ending line end_text &lt;- &quot;^orphan\\\\.$&quot; Why the funny spelling? The start_text and end_text are being specified as a “regex” (sometimes called regular expression) indicating that the word “orphan” is at the very beginning of the line, followed by a period and the end of the line. Regexes are a way of describing patterns. For our purposes, we’ll use them to identify the first and last line of Melville’s work in the Project Gutenberg text. As it happens, there are two instances of “CHAPTER 1.” in Moby Dick. The second is a book within the book. We want to start with the early instance. first_line &lt;- min(grep(start_text, Moby)) last_line &lt;- grep(end_text, Moby) Moby &lt;- Moby[first_line : last_line] We want to break the strings up into individual words. We’ll do this “by hand” because I want to render the text as a simple set of words and punctuation. Steps: Change punctuation so that it is an isolated character. Split up each line by spaces into words. Convert to lower case (because I’m not interested in capitalization). tmp &lt;- Moby characters &lt;- unlist(strsplit(tolower(Moby), split = NULL)) # Step 1 punctuation &lt;- c(&quot;.&quot;, &quot;,&quot;, &quot;;&quot;, &quot;:&quot;, &quot;?&quot;, &quot;!&quot;, &#39;&quot;&#39;, &quot;&#39;&quot;, &quot;&amp;&quot;, &quot;-&quot;, &quot;(&quot;, &quot;)&quot;, &quot;[&quot;, &quot;]&quot;) for (symbol in punctuation) { result &lt;- paste0(&quot; &quot;, symbol, &quot; &quot;) tmp &lt;- gsub(symbol, result, tmp, fixed=TRUE ) } # Step 2 Words &lt;- unlist(strsplit(tmp, split = &quot; &quot;)) # Step 3 Words &lt;- data.frame(word = tolower(Words), stringsAsFactors = FALSE) # Get rid of empty strings # Words &lt;- # Words %&gt;% # filter(word != &quot;&quot;) What are the character frequencies in the book? table(characters) %&gt;% data.frame(stringsAsFactors = FALSE) %&gt;% arrange(desc(Freq)) ## characters Freq ## 1 190500 ## 2 e 115021 ## 3 t 86551 ## 4 a 76496 ## 5 o 68135 ## 6 n 64555 ## 7 i 64384 ## 8 s 63107 ## 9 h 61779 ## 10 r 51160 ## 11 l 42049 ## 12 d 37658 ## 13 u 26316 ## 14 m 22904 ## 15 c 22143 ## 16 w 21774 ## 17 g 20493 ## 18 f 20476 ## 19 , 18947 ## 20 p 16961 ## 21 y 16602 ## 22 b 16600 ## 23 v 8418 ## 24 k 7937 ## 25 . 7385 ## 26 - 5741 ## 27 ; 4143 ## 28 &quot; 2879 ## 29 &#39; 2850 ## 30 ! 1741 ## 31 q 1544 ## 32 j 1061 ## 33 x 1006 ## 34 ? 999 ## 35 z 623 ## 36 ( 201 ## 37 ) 201 ## 38 : 192 ## 39 0 123 ## 40 1 123 ## 41 2 54 ## 42 5 52 ## 43 7 47 ## 44 8 47 ## 45 * 45 ## 46 3 45 ## 47 4 34 ## 48 6 31 ## 49 9 31 ## 50 _ 4 ## 51 [ 2 ## 52 ] 2 ## 53 &amp; 2 ## 54 $ 2 18.2 Most common words Popular &lt;- Words %&gt;% group_by(word) %&gt;% tally() %&gt;% arrange(desc(n)) %&gt;% head(50) 18.3 Most common sequences Sequences &lt;- Words %&gt;% filter(grepl(&quot;[a-zA-Z]&quot;, word)) %&gt;% mutate(two = lead(word, 1), three = lead(two, 1), four = lead(three, 1)) CommonPairs &lt;- Sequences %&gt;% group_by(word, two) %&gt;% tally() %&gt;% ungroup() %&gt;% arrange(desc(n)) Popular triplets Triplets &lt;- Sequences %&gt;% group_by(word, two, three) %&gt;% tally() %&gt;% ungroup() %&gt;% arrange(desc(n)) "],
["part-data-verbs.html", "§ 19 (PART) Data Verbs", " § 19 (PART) Data Verbs "],
["some-resources.html", "§ 20 Some resources", " § 20 Some resources Data scraping: https://github.com/ropensci/user2016-tutorial Text analysis of tweets: http://varianceexplained.org/r/trump-tweets/ "]
]
